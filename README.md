---

# ROB2 Analysis Pipeline

A modular pipeline for extracting structured content from PDFs, performing ROB2 bias assessments with LLMs, and validating meta-analysis datasets.

---

## ğŸ’¡ Overview

This project provides a command-line based processing pipeline that supports:


### âœ… **1. ROB2 risk-of-bias evaluation** (Domain1â€“Domain5 + overall)

Automatically evaluates ROB2 domains using a large language model (e.g., GPT-5).

### âœ… **2. Meta-analysis data validation**

Supports both **binary outcomes** and **continuous outcomes**.

### âœ… **2. Report generate**

### âœ… **4. Configurable runtime**

Using:

* `.env` variables
* Command-line arguments
* Logging to file + console
* Modular task-based execution (`rob`, `report`, `check`)

---

## ğŸ“‚ Project Structure

```
evaluation_Systematic_review/
â”‚
â”œâ”€ data/                    # Input data: text and Excel files
â”‚   â”œâ”€ SR1.txt
â”‚   â”œâ”€ SR1Aguloetal2023.txt
â”‚   â”œâ”€ SR1Hamiltonetal2020.txt
â”‚   â”œâ”€ SR1Hamiltonetal2022.txt
â”‚   â”œâ”€ SR1Krausetal2023.txt
â”‚   â”œâ”€ SR1Mosleyetal2023.txt
â”‚   â”œâ”€ SR1Thomasetal2021.txt
â”‚   â””â”€ SR1Thomasetal2021.xlsx
â”‚
â”œâ”€ prompt/                  # LLM prompt templates and processing functions
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ bin_meta_code.py         # Binary outcome meta-analysis checks
â”‚   â”œâ”€ continue_meta_code.py    # Continuous outcome checks
â”‚   â”œâ”€ extract_info.py          # Extract structured info from text
â”‚   â”œâ”€ meta_analysis.py
â”‚   â”œâ”€ report_generator.py
â”‚   â”œâ”€ ROB2_analysis.py         # Main ROB2 evaluation
â”‚   â”œâ”€ ROB2_analysis_bak.py     # Backup / old ROB2 version
â”‚   â”œâ”€ rr_test.py
â”‚   â””â”€ system_prompt.py         # Base prompts for LLMs
â”‚
â”œâ”€ report/                  # Report generation and plotting
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ check_info.py
â”‚   â”œâ”€ plot_meta_forest.py     # Forest plot generation
â”‚   â”œâ”€ report_info.py          # Extract report sections
â”‚   â”œâ”€ report_to_doc.py        # Generate DOCX reports
â”‚   â””â”€ rob_compare.py          # Compare ROB2 results across studies
â”‚
â”œâ”€ utils/                   # Utility functions
â”‚   â”œâ”€ __init__.py
â”‚   â”œâ”€ alg_util.py
â”‚   â””â”€ content_utils.py       # read/write text, JSON
â”‚
â”œâ”€ vector/                  # Vector search / embeddings
â”‚   â”œâ”€ __init__.py
â”‚   â””â”€ vector_search.py
â”‚
â”œâ”€ main.py                  # Entry point with command-line args
â”œâ”€ .env                     # Environment variables
â””â”€ mylog.log                # Runtime logs

```

---

## âš™ï¸ Installation

### **1. Install dependencies**

```bash
pip install -r requirement.txt
```


```

---

## ğŸ”§ Environment Variables

Create a `.env` file in the root directory:

```
OPENAI_API_KEY=
GEMINI_KEY=
OPENAI_BASE_URL=

```

---

## ğŸš€ How to Run

### **1. Base command**

```bash
python main.py
```

### **2. Override via arguments**

| Argument      | Description                    |
| ------------- | ------------------------------ |
| `--command`   | rob / report / check           |
| `--meta_path` | path to metadata               |
| `--data_path` | input PDF directory            |
| `--save_path` | directory for generated output |

Example:

```bash
python main.py --command rob --data_path ./data --save_path ./save
```

---

## ğŸ§  Commands

### ### **ğŸ”¹ 1. ROB2 Evaluation**

```
--command rob
```

For each file in `data_path`, this will:

* load the text
* extract ROB2-specific info
* run 5 domain evaluations via LLM
* compute overall judgment
* save results as JSON

Output example:

```
/save/study1.json
/save/study2.json
```

---

### ### **ğŸ”¹ 2. Report Info Extraction**

```
--command report
```

This reads a metadata file and extracts sections such as:

* report_1
* report_3_1
* report_3_2
* report_3_3
* report_3_4
* report_3_5

Usage:

```bash
python main.py --command report --save_path ./meta/report.txt
```

---

### ### **ğŸ”¹ 3. Meta-analysis Checking**

```
--command check
```

This command:

* loads extracted meta-analysis data
* determines whether the dataset is binary or continuous
* runs appropriate statistical validation
* outputs corrected values + diagnostics

Output example:

```
SR4_output.json
```

---

## ğŸ“ Logging

Two logging channels are enabled:

1. **Console**
2. **File `mylog.log`**

You will see detailed information for debugging, including:

* current configuration
* file processing progress
* ROB2 domain evaluation results

---

## ğŸ§© Key Functions

### `rob_run(file_path, file, save_path)`

Performs ROB2 domain analysis.

### `extract_report_info()`

Extracts predefined structured items from report text.

### `meta_check()`

Checks consistency of meta-analysis datasets (binary & continuous).

### `read_content()` / `write_str_to_file()`

Basic IO utilities.

---

## ğŸ“¤ Output Example

### ROB2 JSON Output

```json
{
    "domains": [
        { "domain_1": { "judgment": "low" } },
        { "domain_2": { "judgment": "some concerns" } },
        ...
    ],
    "overall": "some concerns"
}
```

---

